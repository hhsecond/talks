{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmymRLn7olYQ"
   },
   "source": [
    "# Deep Learning Workflow\n",
    "Managing DL workflow is always a nightmare. Problems include handling the scale, efficient resource utilization, version controlling the data etc. With the heavily organized Hangar, we can keep the data on check now, not as a blob but as tensors in the data store and version at. The super flexible PyTorch gives us the advantage of prototyping faster and iterate smoother. This model prototype can now be pushed to RedisAI, the highly optimized production runtime, as optimized torchscript and scale the serving to multi node redis cluster or make it highly available with redis sentinel\n",
    "\n",
    "This tutorial is devided into three parts\n",
    "1. Hangar\n",
    "2. PyTorch / Tensorflow or anyother framework that has an ONNX export\n",
    "3. RedisAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "957-mktQoqff"
   },
   "source": [
    "## Hangar\n",
    "This tutorial will review the first steps of working with a hangar repository.\n",
    "To fit with the beginner's theme, we will use the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "-3NqcMbHn8gw",
    "outputId": "888675f3-0d82-4ed8-da0b-3b3d2cf2e5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorwerk/hangar-py\n",
      "  Cloning https://github.com/tensorwerk/hangar-py to /tmp/pip-req-build-siue6qt_\n",
      "  Running command git clone -q https://github.com/tensorwerk/hangar-py /tmp/pip-req-build-siue6qt_\n",
      "Requirement already satisfied (use --upgrade to upgrade): hangar==0.1.1 from git+https://github.com/tensorwerk/hangar-py in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages\n",
      "Requirement already satisfied: click in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (7.0)\n",
      "Requirement already satisfied: lmdb==0.94 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (0.94)\n",
      "Requirement already satisfied: h5py==2.9.0 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (2.9.0)\n",
      "Requirement already satisfied: numpy in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (5.1.1)\n",
      "Requirement already satisfied: msgpack==0.6.1 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (0.6.1)\n",
      "Requirement already satisfied: blosc in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (1.8.1)\n",
      "Requirement already satisfied: grpcio in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (1.21.1)\n",
      "Requirement already satisfied: grpcio_tools in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (1.21.1)\n",
      "Requirement already satisfied: tqdm in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (4.32.2)\n",
      "Requirement already satisfied: wrapt in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from hangar==0.1.1) (1.11.2)\n",
      "Requirement already satisfied: six in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from h5py==2.9.0->hangar==0.1.1) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0.post1 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from grpcio_tools->hangar==0.1.1) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from protobuf>=3.5.0.post1->grpcio_tools->hangar==0.1.1) (41.0.1)\n",
      "Building wheels for collected packages: hangar\n",
      "  Building wheel for hangar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jhkhfa3p/wheels/41/92/9e/a01c44b33015b54b966237badb395ec6ff104b78676e83c1aa\n",
      "Successfully built hangar\n",
      "Requirement already satisfied: matplotlib in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from matplotlib) (1.16.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Collecting git+https://github.com/redisai/redisai-py@onnxruntime\n",
      "  Cloning https://github.com/redisai/redisai-py (to revision onnxruntime) to /tmp/pip-req-build-20qme0t6\n",
      "  Running command git clone -q https://github.com/redisai/redisai-py /tmp/pip-req-build-20qme0t6\n",
      "  Running command git checkout -b onnxruntime --track origin/onnxruntime\n",
      "  Switched to a new branch 'onnxruntime'\n",
      "  Branch 'onnxruntime' set up to track remote branch 'onnxruntime' from 'origin'.\n",
      "Requirement already satisfied (use --upgrade to upgrade): redisai==0.3.0 from git+https://github.com/redisai/redisai-py@onnxruntime in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages\n",
      "Requirement already satisfied: redis in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from redisai==0.3.0) (3.2.1)\n",
      "Requirement already satisfied: hiredis in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from redisai==0.3.0) (1.0.0)\n",
      "Requirement already satisfied: rmtest in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from redisai==0.3.0) (0.7.0)\n",
      "Building wheels for collected packages: redisai\n",
      "  Building wheel for redisai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-1wbzg6kq/wheels/29/c6/9e/f12d903a5dd2a86d75ba2a2683a74ec1b7ee021f3c4fb5a2e0\n",
      "Successfully built redisai\n",
      "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-linux_x86_64.whl in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (from torch==1.1.0) (1.16.4)\n",
      "Requirement already satisfied: tqdm in /home/hhsecond/anaconda3/envs/dlblr/lib/python3.7/site-packages (4.32.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio>=1.21.1\n",
    "!pip install git+https://github.com/tensorwerk/hangar-py\n",
    "!pip install matplotlib\n",
    "!pip install git+https://github.com/redisai/redisai-py@onnxruntime\n",
    "!pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6Kul_B8pBpk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hangar import Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Interacting with a Hangar Repository\n",
    "\n",
    "Hangar is designed to \"just make sense\" in every operation you have to perform. As such, there is a single interface which all interaction begins with:\n",
    "the `Repository` object. \n",
    "\n",
    "Weather a hangar repository exists at the path you specify or not, just tell hangar where it should live!\n",
    "\n",
    "#### Intitializing a repository\n",
    "\n",
    "The first time you want to work with a new repository, the `init()` method must be called. This is where you provide hangar with your name and email address (to be used in the commit log), as well as implicitly confirming that you do want to create the underlying data files hangar uses on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ozH8Dw6WqC4k",
    "outputId": "54db52a9-0450-4ad1-ce4d-d346ea1819c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘myhangarrepo’: File exists\n",
      "Hangar Repo initialized at: myhangarrepo/__hangar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'myhangarrepo/__hangar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir myhangarrepo\n",
    "repo = Repository(path='myhangarrepo')\n",
    "repo.init(user_name='Sherin Thomas', user_email='sherin@gmail.com', remove_old=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "siC6_-FZqm6S",
    "outputId": "38c3fcca-66fd-4c13-f1cb-bf886ec4c9b3"
   },
   "outputs": [],
   "source": [
    "repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QLtXeXesq3pA",
    "outputId": "2285a143-9a91-4f7a-c21a-3ff7bd17952b"
   },
   "outputs": [],
   "source": [
    "repo.writer_lock_held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.repo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjNdOpEuq7tI"
   },
   "outputs": [],
   "source": [
    "# data link\n",
    "# https://drive.google.com/drive/folders/1zYdhNN4s5QnqGHRN632hXvfCt4OxsF0l?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T-HEZ75734Gc",
    "outputId": "8d02af33-756c-461b-c485-6623adae5044"
   },
   "outputs": [],
   "source": [
    "datapath = \"mnist_data\"\n",
    "import os\n",
    "os.listdir(datapath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvjaDyVB6MWT"
   },
   "outputs": [],
   "source": [
    "target = np.load(os.path.join(datapath, 'target1.npy'))\n",
    "data = np.load(os.path.join(datapath, 'data1.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DhCRAmRH6v9_",
    "outputId": "60f592a8-4426-4470-e306-054294dcc71d"
   },
   "outputs": [],
   "source": [
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the repo for writing\n",
    "\n",
    "A repository can be checked out in two modes: \n",
    "\n",
    "1) **write-enabled**: applies all operations to the staging area's current state. Only one write-enabled checkout can be active at a different time, must be closed upon last use, or manual intervention will be needed to remove the writer lock. \n",
    "    \n",
    "2) **read-only**: checkout a commit or branch to view repository state as it existed at that point in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPEE6PlT7Ra3"
   },
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'co' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-abcc075f5f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'co' is not defined"
     ]
    }
   ],
   "source": [
    "co.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'co' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f55801262a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'co' is not defined"
     ]
    }
   ],
   "source": [
    "co.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before data can be added to a repository, a dataset must be initialized. \n",
    "\n",
    "A Dataset is a named grouping of data samples where each sample shares a number of similar attributes and array properties:\n",
    "\n",
    "https://hangar-py.readthedocs.io/en/latest/concepts.html#how-hangar-thinks-about-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nkGhBL207w-E",
    "outputId": "1f078eef-4de2-41f4-e932-cb5c4b72b8d8"
   },
   "outputs": [],
   "source": [
    "data_dset = co.datasets.init_dataset('mnist_data', shape=(28, 28), dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction \n",
    "\n",
    "When a dataset is initialized, a dataset accessor object will be returned, however, depending on your use case, this may or may not be the most convenient way to access a dataset.\n",
    "\n",
    "In general, we have implemented a full `dict` mapping interface ontop of all object. To access the `'mnist_training_images'` dataset you can just use a dict style access like the following (note: if operating in ipython/jupyter, the dataset keys will autocomplete for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KMBRM6NC8GmD",
    "outputId": "de50d22d-c580-42d8-f00a-b225c6d5911e"
   },
   "outputs": [],
   "source": [
    "co.datasets['mnist_data'] == data_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ead4ClkJ8brB",
    "outputId": "37d04356-6eca-4482-cd0a-466086b8ff21"
   },
   "outputs": [],
   "source": [
    "target_dset = co.datasets.init_dataset('mnist_target', prototype=target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "h3W528TqAWFh",
    "outputId": "6418fd0e-6fe8-4ea7-b711-8e374c9e11c4"
   },
   "outputs": [],
   "source": [
    "co.commit('datasets init')\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3zR6cnOBeo1"
   },
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)\n",
    "data_dset = co.datasets['mnist_data']\n",
    "target_dset = co.datasets['mnist_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "Once you've completed an interactive exploration, be sure to use the context manager form of the `.add` and `.get` methods! \n",
    "\n",
    "In order to make sure that all your data is always safe in Hangar, the backend dilligently ensures that all contexts are opened and closed appropriatly. \n",
    "\n",
    "When you use the context manager form of a dataset object, we can offload a significat amount of work to the python runtime, and dramatically increase read and write speeds.\n",
    "\n",
    "Most datasets we've tested see an increased throughput differential of ~250% for writes and ~300% for reads when comparing using the context manager form vs the naked form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "msLE68rrt2il",
    "outputId": "ca7e9557-77b3-4896-879c-1a48ac7f8bd3"
   },
   "outputs": [],
   "source": [
    "with data_dset, target_dset:  # You don't really need this CM if you are not worried about perf\n",
    "  for i in tqdm(range(len(data))):\n",
    "    sample_name = str(i)\n",
    "    data_dset[sample_name] = data[i]\n",
    "    target_dset[sample_name] = np.array(target[i])\n",
    "co.commit('dataset curation: stage 1')\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout()\n",
    "dset = co.datasets['mnist_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1' in dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(dset.values()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dset.items():\n",
    "    print(key)\n",
    "    plt.imshow(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dset['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)\n",
    "co.metadata['DataSource'] = \"Sherin\"\n",
    "co.commit(\"Added source\")\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety from python \"oddities\" is built in Hangar's very essense.\n",
    "\n",
    "- Unknown to the user, Hangar does not actually allow `dataset` or `metadata` objects to be directly referenced in application code.\n",
    "- What you actually get is a `weakref ObjectProxy`. Though semantically identicaly, only Hangar keeps strong references to it's accessors.\n",
    "- When a Hangar object no longer has permissions to act, the `ObjectProxy` \"self destructs\".\n",
    "- Any introspection/call/modification by application code immediatly raises an exception to let you know you're dealing with something which is out of date! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)\n",
    "data_dset = co.datasets['mnist_data']\n",
    "co.close()\n",
    "data_dset['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you put in is what you get out\n",
    "All data is hashed by cryptographically secure hash algorithms (blake2b with 20byte digest length)\n",
    "A commit is entirely self sufficient, and it's hash depends on the hash of previous references\n",
    "For performance reasons, data hash is only calculated / verified when:\n",
    "\n",
    "a sample is added to a dataset\n",
    "data is fetched from a remote repo\n",
    "data is sent to a remote repo\n",
    "During regular read access, data integrity is ensured via fletcher32 / crc32 checksums\n",
    "\n",
    "Backend store utilities provide well validated, trusted, and performant implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching & Merging\n",
    "- Time travel through the historical evolution of a dataset\n",
    "- Zero-cost Branching to enable exploratory analysis and collaboration\n",
    "- Cheap Merging to build datasets over time (with multiple collaborators)\n",
    "- Completely abstracted organization and management of data files on disk\n",
    "- Ability to only retrieve a small portion of the data (as needed) while still maintaining complete historical record\n",
    "- Ability to push and pull changes directly to collaborators or a central server (ie a truly distributed version control system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.create_branch('stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2 = repo.checkout(branch_name='stage2', write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.load(os.path.join(datapath, 'target2.npy'))\n",
    "data = np.load(os.path.join(datapath, 'data2.npy'))\n",
    "target.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with co2.datasets['mnist_data'] as ddset, co2.datasets['mnist_target'] as tdset:\n",
    "    current_index = len(ddset)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        sample_name = str(current_index + i)\n",
    "        ddset[sample_name] = data[i]\n",
    "        tdset[sample_name] = np.array(target[i])\n",
    "co2.metadata['DataSource'] = \"Somebody else\"\n",
    "co2.commit('Data curation: stage2')\n",
    "co2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.list_branch_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.log(branch_name='stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy commit to make the diversion\n",
    "co.metadata['RandomeKey'] = \"RandomValue\"\n",
    "co.commit(\"Dummy metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not a good idea to run this now\n",
    "# from pprint import pprint\n",
    "# pprint(co.diff.branch(\"stage2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.merge(\"Merging stage2\", dev_branch='stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo._details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Disclosure\n",
    "\n",
    "Hangar is an early stage products, none of the core developers have any significant cryptography or security background/experience. While efforts have been made to secure application data, we are not comfortable calling Hangar a `cryptographically secure utility` until a formal security and design review by domain experts has been performed. \n",
    "\n",
    "We are actively looking for help in this area, if you are interested in contributing, please let us know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's pending\n",
    "- Remote Hangar Repository\n",
    "- Import & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "- Dynamic Graph\n",
    "- torch.nn\n",
    "- Datasets & Dataloaders\n",
    "- Training\n",
    "    - Autograd\n",
    "    - Optimization\n",
    "- Validation\n",
    "- Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    img /= 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we are not using that for now\n",
    "class HangarDataset:\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that gives access to hangar dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        if len(data) != len(target):\n",
    "            raise RuntimeError(\"Length of data and target does not match\")\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Since our sample names in hangar repository is str(index),\n",
    "        we can do str(key) to figure out the sample name\n",
    "        \"\"\"\n",
    "        sample_name = str(key)\n",
    "        normalized_img = normalize(self.data[sample_name].reshape(1, 1, 28, 28))\n",
    "        target = self.target[sample_name].reshape(1)\n",
    "        return normalized_img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout()\n",
    "ddset = co.datasets['mnist_data']\n",
    "tdset = co.datasets['mnist_target']\n",
    "hangar_dset = HangarDataset(ddset, tdset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training not on batch = not good\n",
    "# not shuffled = not good\n",
    "for i in tqdm(range(len(hangar_dset))):\n",
    "    data, target = hangar_dset[i]\n",
    "    data = torch.from_numpy(data).to(device)\n",
    "    target = torch.from_numpy(target).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i and i % 5000 == 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Testing & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RedisAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "con = redis.Redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set('foo', 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.get('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redisai as rai\n",
    "traced_model = torch.jit.trace(model, data)\n",
    "rai.save_model(traced_model, 'mnist.pt')\n",
    "del traced_model\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model & tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rai.load_model('mnist.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = rai.BlobTensor.from_numpy(data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with Redis & RedisAI server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = rai.Client(host='localhost', port=6379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.tensorset('input', tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.modelset('model', rai.Backend.torch, rai.Device.cpu, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.modelrun('model', 'input', 'output')\n",
    "# if you have more input and output tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = con.tensorget('output')\n",
    "output = con.tensorget('output', as_type=rai.BlobTensor)\n",
    "output.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_numpy().argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIPTing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "def first_script(arr1, arr2):\n",
    "    return (arr1 / 2) @ arr2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.scriptset('script', rai.Device.cpu, script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = rai.Tensor(value=[8, 8, 8, 8, 8, 8], shape=(3, 2), dtype=rai.DType.int32)\n",
    "np_arr = np.array((2, 2), dtype=np.int32).reshape(2, 1)\n",
    "arr2 = rai.BlobTensor.from_numpy(np_arr)\n",
    "con.tensorset('dummyarr1', arr1)\n",
    "con.tensorset('dummyarr2', arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.scriptrun('script', 'first_script', ['dummyarr1', 'dummyarr2'], 'scriptout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.tensorget('scriptout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.tensorget('scriptout').value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's pending\n",
    "- [Replication & failover](https://github.com/RedisAI/redisai-examples/tree/master/sentinel)\n",
    "- [Other clients](https://github.com/RedisAI/redisai-examples)\n",
    "- [Other backends](https://github.com/RedisAI/redisai-examples/tree/master/python_client)\n",
    "- Keep data local\n",
    "- [RedisEdge](https://github.com/RedisGears/EdgeRealtimeVideoAnalytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "- [Hangar](https://github.com/tensorwerk/hangar-py)\n",
    "- [PyTorch](https://pytorch.org)\n",
    "- [RedisAI](https://github.com/RedisAI/RedisAI)\n",
    "- [This example](https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DeepLearningWorkflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
